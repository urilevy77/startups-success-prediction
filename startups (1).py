# -*- coding: utf-8 -*-
"""startups.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYSnxsaSn5PhzIPLd7p4Ir1Eo6jTSNzG
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt
import seaborn as sns
from datetime import date
from scipy import stats
from scipy.stats import norm, skew #for some statistics

import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.colors import n_colors
from plotly.subplots import make_subplots

import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline
pd.set_option("display.max_columns",None)
pd.set_option("display.max_rows",None)

from collections import Counter
import datetime
import wordcloud
import json

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

def preprocessing(df):
    """
    Comprehensive preprocessing function for startup data

    Args:
        df (pd.DataFrame): Raw startup dataset

    Returns:
        pd.DataFrame: Preprocessed dataset ready for modeling
    """

    # Create a copy to avoid modifying original data
    df = df.copy()

    print("=== ENHANCED DATA PREPROCESSING ===")
    print("Following ML Project Guide - Data Preprocessing Section\n")

    # ========================================
    # 1. COMPLETE MISSING VALUE ANALYSIS
    # ========================================
    print("1. MISSING VALUE ANALYSIS")
    print("-" * 40)

    def analyze_missing_values(df):
        """
        Comprehensive missing value analysis with documentation
        """
        missing_summary = []

        for column in df.columns:
            missing_count = df[column].isnull().sum()
            if missing_count > 0:
                missing_pct = (missing_count / len(df)) * 100
                dtype = str(df[column].dtype)
                missing_summary.append({
                    'Column': column,
                    'Missing_Count': missing_count,
                    'Missing_Percentage': f"{missing_pct:.2f}%",
                    'Data_Type': dtype
                })

        if missing_summary:
            missing_df = pd.DataFrame(missing_summary)
            missing_df = missing_df.sort_values('Missing_Count', ascending=False)
            print("Missing Values Found:")
            print(missing_df.to_string(index=False))
            print(f"\nTotal columns with missing values: {len(missing_summary)} out of {len(df.columns)}")
        else:
            print("‚úÖ No missing values found in dataset")

        return missing_summary

    # Analyze missing values
    missing_analysis = analyze_missing_values(df)

    print("\n" + "="*50)
    print("MISSING VALUE HANDLING STRATEGY")
    print("="*50)

    # ========================================
    # 2. ENHANCED MISSING VALUE HANDLING
    # ========================================

    # Strategy 1: Date columns - Fill with business logic
    print("\nüìÖ DATE COLUMN HANDLING:")
    print("‚Ä¢ closed_at: Fill with 2014-01-01 (business assumption: ongoing startups)")
    print("  Reasoning: Startups without closed_at are likely still operating")
    print("‚Ä¢ Milestone dates: Keep NaN (indicates no milestones achieved)")
    print("  Reasoning: Missing milestone dates are meaningful (no milestones)")

    # Convert date columns with proper handling
    df['closed_at'] = pd.to_datetime(df['closed_at'], errors='coerce')
    df['closed_at'].fillna(pd.Timestamp("2014-01-01"), inplace=True)
    df['founded_at'] = pd.to_datetime(df['founded_at'], errors='coerce')

    # handle missing values in milestones and age
    if 'age_first_milestone_year' in df.columns:
        df['age_first_milestone_year'] = df['age_first_milestone_year'].fillna(0)
    if 'age_last_milestone_year' in df.columns:
        df['age_last_milestone_year'] = df['age_last_milestone_year'].fillna(0)

    # Strategy 2: Handle irrelevant columns
    print("\nüóÇÔ∏è IRRELEVANT COLUMN HANDLING:")
    print("‚Ä¢ Unnamed: 6: High missing percentage - likely formatting artifact, safe to drop")
    print("‚Ä¢ state_code.1: Low missing percentage - duplicate of state_code, will be dropped")

    # Strategy 3: Milestone age columns - Keep NaN as meaningful
    print("\nüìä MILESTONE AGE COLUMNS:")
    print("‚Ä¢ age_first_milestone_year & age_last_milestone_year: Missing values")
    print("‚Ä¢ Decision: Fill with 0 (meaningful - indicates no milestones)")
    print("‚Ä¢ Reasoning: Missing milestone data represents business reality")

    # ========================================
    # 3. FEATURE ENGINEERING
    # ========================================
    print("\n" + "="*50)
    print("FEATURE ENGINEERING")
    print("="*50)

    # Age calculation
    print("\nüïê AGE FEATURE CREATION:")
    df['age'] = ((df['closed_at'] - df['founded_at']).dt.days / 365).round(2)
    print("‚Ä¢ Created 'age' feature: (closed_at - founded_at) in years")

    # Age categories
    print("\nüìä AGE CATEGORIZATION:")
    df['age_category'] = pd.cut(df['age'],
                               bins=[0, 3, 7, 15, float('inf')],
                               labels=['young', 'growing', 'mature', 'veteran'])
    print("‚Ä¢ Age categories: young (0-3), growing (3-7), mature (7-15), veteran (15+)")

    # One-hot encode age categories
    age_dummies = pd.get_dummies(df['age_category'], prefix='age')
    df = pd.concat([df, age_dummies], axis=1)
    print("‚Ä¢ One-hot encoded age categories")

    # Remove negative values in age columns
    print("\nüîç DATA QUALITY FILTERING:")
    age_columns = [
        'age_first_funding_year', 'age_last_funding_year',
        'age_first_milestone_year', 'age_last_milestone_year'
    ]

    for col in age_columns:
        if col in df.columns:
            before_count = len(df)
            df = df[(df[col].isnull()) | (df[col] >= 0)]
            removed = before_count - len(df)
            if removed > 0:
                print(f"‚Ä¢ Removed {removed} rows with negative {col}")

    # ========================================
    # 4. TARGET VARIABLE ANALYSIS
    # ========================================
    print("\n" + "="*50)
    print("TARGET VARIABLE ANALYSIS")
    print("="*50)

    def analyze_target_balance(df, target_col='labels'):
        """Analyze target variable distribution and class balance"""

        if target_col not in df.columns:
            print(f"‚ö†Ô∏è Target column '{target_col}' not found in dataset")
            return None

        print(f"\nüéØ TARGET VARIABLE: '{target_col}'")
        target_counts = df[target_col].value_counts().sort_index()

        print("\nClass Distribution:")
        for class_val, count in target_counts.items():
            percentage = (count / len(df)) * 100
            print(f"  Class {class_val}: {count:,} samples ({percentage:.2f}%)")

        # Calculate imbalance ratio
        max_class = target_counts.max()
        min_class = target_counts.min()
        imbalance_ratio = max_class / min_class

        print(f"\nImbalance Ratio: {imbalance_ratio:.2f}:1")

        # Class balancing recommendation
        print("\n" + "="*30)
        print("CLASS BALANCE ANALYSIS")
        print("="*30)

        if imbalance_ratio <= 3:
            print("‚úÖ DATASET IS WELL BALANCED")
            print(f"‚Ä¢ Imbalance ratio ({imbalance_ratio:.2f}:1) is within acceptable threshold (<3:1)")
            print("‚Ä¢ Both classes are adequately represented")
            print("‚Ä¢ Natural class distribution preserves real-world patterns")
            print("‚Ä¢ Avoids overfitting risks from synthetic sample generation")
            print("‚Ä¢ No need for SMOTE, ADASYN, or class_weight adjustments")

            print(f"\nüìä Distribution Analysis:")
            print(f"‚Ä¢ Majority class: {(max_class/len(df)*100):.1f}% of data")
            print(f"‚Ä¢ Minority class: {(min_class/len(df)*100):.1f}% of data")
            print("‚Ä¢ Both classes have sufficient samples for robust learning")

        else:
            print(f"‚ö†Ô∏è MODERATE IMBALANCE DETECTED")
            print(f"‚Ä¢ Consider balancing techniques for ratio {imbalance_ratio:.2f}:1")

        return imbalance_ratio

    # Analyze target variable if it exists
    if 'labels' in df.columns:
        target_balance = analyze_target_balance(df, 'labels')
    else:
        target_balance = None
        print("‚ö†Ô∏è No 'labels' column found for target analysis")

    # ========================================
    # 5. FINAL CLEANUP
    # ========================================
    print("\n" + "="*50)
    print("FINAL PREPROCESSING CLEANUP")
    print("="*50)

    # Drop irrelevant columns
    columns_to_drop = [
        'id', 'object_id', 'Unnamed: 6', 'Unnamed: 0',
        'latitude', 'longitude', 'state_code.1'
    ]

    print(f"\nüóëÔ∏è DROPPING IRRELEVANT COLUMNS:")
    dropped_cols = [col for col in columns_to_drop if col in df.columns]
    df = df.drop(columns=dropped_cols)
    print(f"‚Ä¢ Dropped {len(dropped_cols)} columns: {dropped_cols}")
    print(f"‚Ä¢ Remaining columns: {len(df.columns)}")

    print(f"\nüìä PREPROCESSING SUMMARY:")
    print(f"‚Ä¢ Final dataset shape: {df.shape}")
    print(f"‚Ä¢ Features: {df.shape[1] - (1 if 'labels' in df.columns else 0)}")
    print(f"‚Ä¢ Samples: {df.shape[0]:,}")
    if target_balance:
        print(f"‚Ä¢ Target balance: {target_balance:.2f}:1 (No balancing needed)")

    print("\n‚úÖ PREPROCESSING COMPLETE")
    print("="*50)
    print("üìã NEXT STEPS:")
    print("‚Ä¢ Proceed to Feature Selection")
    print("‚Ä¢ Apply Feature Scaling")
    print("‚Ä¢ Begin Model Development")
    print("="*50)

    return df

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

def feature_engineering_selection(df, target='labels'):
    """
    Comprehensive feature engineering and selection function for startup data

    Args:
        df (pd.DataFrame): Preprocessed startup dataset
        target (str): Name of target column (default: 'labels')

    Returns:
        tuple: (df_processed, selected_features, target_series)
            - df_processed: DataFrame with engineered features
            - selected_features: List of selected feature names
            - target_series: Target variable series
    """

    # Create a copy to avoid modifying original data
    df = df.copy()

    print("=== ENHANCED FEATURE ENGINEERING & SELECTION ===")
    print("Following ML Project Guide - Feature Engineering & Selection Section\n")

    # ========================================
    # 1. FEATURE ENGINEERING WITH DOCUMENTATION
    # ========================================
    print("=" * 60)
    print("FEATURE ENGINEERING WITH BUSINESS RATIONALE")
    print("=" * 60)

    print("\nüè¢ BUSINESS LOGIC FEATURES:")
    print("Following domain expertise and startup success patterns\n")

    # Check if required columns exist for feature engineering
    required_cols = ['has_roundA', 'has_roundB', 'has_roundC', 'has_roundD', 'has_VC', 'has_angel']
    missing_cols = [col for col in required_cols if col not in df.columns]

    if missing_cols:
        print(f"‚ö†Ô∏è Warning: Missing columns for feature engineering: {missing_cols}")
        print("Creating placeholder columns with zeros...")
        for col in missing_cols:
            df[col] = 0

    # Existing business logic features with enhanced documentation
    print("üìä FUNDING SUCCESS INDICATORS:")

    df['has_all_rounds'] = ((df['has_roundA'] + df['has_roundB'] +
                            df['has_roundC'] + df['has_roundD']) == 4).astype(int)
    print("‚Ä¢ has_all_rounds: Complete funding progression indicator")
    print("  Business Rationale: Startups completing all funding rounds (A-D)")
    print("  show systematic growth and investor confidence progression")

    df['has_any'] = np.where((df['has_roundA'] == 1) | (df['has_roundB'] == 1) |
                            (df['has_roundC'] == 1) | (df['has_roundD'] == 1), 1, 0)
    print("\n‚Ä¢ has_any: Basic funding success indicator")
    print("  Business Rationale: Any successful funding round indicates")
    print("  market validation and investor interest")

    df['has_Investor'] = np.where((df['has_VC'] == 1) | (df['has_angel'] == 1), 1, 0)
    print("\n‚Ä¢ has_Investor: Professional backing indicator")
    print("  Business Rationale: VC or angel backing provides expertise,")
    print("  networks, and credibility beyond just capital")

    df['has_Seed'] = np.where((df['has_any'] == 0) & (df['has_Investor'] == 1), 1, 0)
    print("\n‚Ä¢ has_Seed: Early-stage potential indicator")
    print("  Business Rationale: Has investors but no formal rounds suggests")
    print("  seed/pre-seed stage with potential for future funding")

    df['invalid_startup'] = np.where((df['has_any'] == 0) & (df['has_VC'] == 0) &
                                    (df['has_angel'] == 0), 1, 0)
    print("\n‚Ä¢ invalid_startup: High-risk startup indicator")
    print("  Business Rationale: No funding or investors indicates")
    print("  either very early stage or struggling startup")

    print(f"\nüìà FEATURE ENGINEERING SUMMARY:")
    print(f"‚Ä¢ Created {5} business logic features")
    print("‚Ä¢ All features based on startup funding lifecycle domain knowledge")
    print("‚Ä¢ Features capture different stages of startup maturity and success")

    # ========================================
    # 2. DATA PREPARATION
    # ========================================
    print("\n" + "=" * 60)
    print("DATA PREPARATION FOR FEATURE SELECTION")
    print("=" * 60)

    # Drop columns with clear business reasoning
    columns_to_drop = [
        'age', 'age_category',  # Processed into encoded features
        'name', 'state_code', 'category_code', 'zip_code',  # Categorical/ID columns
        'founded_at', 'closed_at', 'first_funding_at', 'last_funding_at',  # Raw dates
        'city'  # Geographic detail
    ]

    print(f"\nüóëÔ∏è DROPPING NON-PREDICTIVE COLUMNS:")
    existing_drop_cols = [col for col in columns_to_drop if col in df.columns]
    print(f"‚Ä¢ Removed {len(existing_drop_cols)} columns: {existing_drop_cols}")
    print("‚Ä¢ Rationale: Keep numerical features and engineered business logic")

    df_train = df.drop(columns=existing_drop_cols)

    # Check if target exists
    if target not in df_train.columns:
        raise ValueError(f"Target column '{target}' not found in dataset. Available columns: {df_train.columns.tolist()}")

    # Separate features and target
    other_targets = ['status'] if 'status' in df_train.columns else []
    X = df_train.drop(columns=[target] + other_targets, errors="ignore")
    y = df_train[target]

    print(f"\nüìä DATASET PREPARED:")
    print(f"‚Ä¢ Features: {X.shape[1]}")
    print(f"‚Ä¢ Samples: {X.shape[0]}")
    print(f"‚Ä¢ Target classes: {y.nunique()}")

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    # ========================================
    # 3. COMPREHENSIVE FEATURE SELECTION
    # ========================================
    print("\n" + "=" * 60)
    print("COMPREHENSIVE FEATURE SELECTION STRATEGY")
    print("=" * 60)

    print("\nüéØ SELECTION STRATEGY RATIONALE:")
    print("‚Ä¢ Method 1: Tree-based importance (XGBoost) - Captures non-linear relationships")
    print("‚Ä¢ Method 2: Cross-validated importance - Validates feature stability")
    print("‚Ä¢ Threshold: 'median' - Balanced approach removing bottom 50% features")

    # ========================================
    # Method 1: Tree-based Feature Selection (Enhanced)
    # ========================================
    print("\n" + "-" * 40)
    print("METHOD 1: TREE-BASED FEATURE SELECTION")
    print("-" * 40)

    # Train XGBoost model
    xgb_model = XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42
    )

    print("üå≥ Training XGBoost for feature importance...")
    xgb_model.fit(X_train, y_train)

    # Feature selection with median threshold
    selector_xgb = SelectFromModel(estimator=xgb_model, threshold="median", prefit=True)
    X_train_xgb = selector_xgb.transform(X_train)
    X_test_xgb = selector_xgb.transform(X_test)

    selected_features_xgb = X_train.columns[selector_xgb.get_support()]
    print(f"‚úÖ XGBoost selected {len(selected_features_xgb)} features:")
    print(f"‚Ä¢ {selected_features_xgb.tolist()}")

    # ========================================
    # Method 2: Cross-Validated Feature Importance
    # ========================================
    print("\n" + "-" * 40)
    print("METHOD 2: CROSS-VALIDATED FEATURE IMPORTANCE")
    print("-" * 40)

    print("üîÑ Performing cross-validated feature importance...")

    def cross_validated_feature_importance(X, y, model, cv_folds=5):
        """Calculate feature importance across multiple CV folds"""

        feature_importance_scores = defaultdict(list)
        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]

            # Train model on fold
            model.fit(X_fold_train, y_fold_train)

            # Get feature importance
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
            else:
                importances = np.abs(model.coef_[0])

            # Store importance for each feature
            for feature, importance in zip(X.columns, importances):
                feature_importance_scores[feature].append(importance)

        # Calculate mean and std of importance across folds
        cv_importance = {}
        for feature, scores in feature_importance_scores.items():
            cv_importance[feature] = {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'stability': 1 - (np.std(scores) / (np.mean(scores) + 1e-8))
            }

        return cv_importance

    # Calculate cross-validated importance
    cv_importance = cross_validated_feature_importance(X_train, y_train,
                                                     RandomForestClassifier(n_estimators=50, random_state=42))

    # Sort by mean importance
    cv_sorted = sorted(cv_importance.items(), key=lambda x: x[1]['mean'], reverse=True)

    print("üìä Cross-validated feature importance (Top 10):")
    for i, (feature, stats) in enumerate(cv_sorted[:10]):
        print(f"{i+1:2d}. {feature:<25} | Importance: {stats['mean']:.4f} ¬± {stats['std']:.4f} | Stability: {stats['stability']:.3f}")

    # Select top features based on CV importance
    n_features_to_select = len(selected_features_xgb)
    top_cv_features = [feat for feat, _ in cv_sorted[:n_features_to_select]]
    print(f"\n‚úÖ CV-based selected {len(top_cv_features)} features:")
    print(f"‚Ä¢ {top_cv_features}")

    # ========================================
    # 3. FEATURE IMPORTANCE VISUALIZATION
    # ========================================
    print("\n" + "=" * 60)
    print("FEATURE IMPORTANCE VISUALIZATION")
    print("=" * 60)

    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Comprehensive Feature Selection Analysis', fontsize=16, fontweight='bold')

    # Plot 1: XGBoost Feature Importance
    ax1 = axes[0, 0]
    xgb_importance = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)
    top_xgb = xgb_importance.head(15)
    bars1 = ax1.barh(range(len(top_xgb)), top_xgb.values, color='lightblue', edgecolor='navy', alpha=0.7)
    ax1.set_yticks(range(len(top_xgb)))
    ax1.set_yticklabels(top_xgb.index, fontsize=10)
    ax1.set_xlabel('XGBoost Feature Importance', fontweight='bold')
    ax1.set_title('XGBoost Feature Ranking', fontweight='bold')
    ax1.grid(axis='x', alpha=0.3)

    # Highlight selected features
    for i, (idx, importance) in enumerate(zip(range(len(top_xgb)), top_xgb.values)):
        if top_xgb.index[i] in selected_features_xgb:
            bars1[i].set_color('orange')
            bars1[i].set_alpha(0.9)

    # Plot 2: Cross-Validated Importance with Error Bars
    ax2 = axes[0, 1]
    cv_means = [stats['mean'] for _, stats in cv_sorted[:15]]
    cv_stds = [stats['std'] for _, stats in cv_sorted[:15]]
    cv_features = [feat for feat, _ in cv_sorted[:15]]

    bars2 = ax2.barh(range(len(cv_features)), cv_means, xerr=cv_stds,
                    color='lightcoral', edgecolor='darkred', alpha=0.7, capsize=3)
    ax2.set_yticks(range(len(cv_features)))
    ax2.set_yticklabels(cv_features, fontsize=10)
    ax2.set_xlabel('CV Feature Importance (Mean ¬± Std)', fontweight='bold')
    ax2.set_title('Cross-Validated Feature Importance', fontweight='bold')
    ax2.grid(axis='x', alpha=0.3)

    # Plot 3: Feature Selection Overlap Analysis
    set_xgb = set(selected_features_xgb)
    set_cv = set(top_cv_features)
    overlap_all = set_xgb & set_cv

    counts = {
        'XGBoost Only': len(set_xgb - set_cv),
        'CV Only': len(set_cv - set_xgb),
        'XGB + CV': len(overlap_all),
    }

    ax3 = axes[1, 0]
    bars3 = ax3.bar(counts.keys(), counts.values(), color=['skyblue', 'lightcoral', 'gold'], edgecolor='black')

    for bar in bars3:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2, height + 0.2, str(height),
                ha='center', va='bottom', fontsize=12, fontweight='bold')

    ax3.set_ylabel("Number of Features", fontweight='bold')
    ax3.set_title("Feature Selection Method Overlap", fontweight='bold')
    ax3.grid(axis='y', linestyle='--', alpha=0.5)

    # Plot 4: Feature Distribution
    ax4 = axes[1, 1]
    feature_types = []
    business_features = ['has_all_rounds', 'has_any', 'has_Investor', 'has_Seed', 'invalid_startup']

    for feature in selected_features_xgb:
        if feature in business_features:
            feature_types.append('Business Logic')
        elif 'age_' in feature:
            feature_types.append('Age Categories')
        elif any(x in feature for x in ['has_', 'is_']):
            feature_types.append('Binary Features')
        else:
            feature_types.append('Other')

    type_counts = pd.Series(feature_types).value_counts()
    ax4.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%', startangle=90)
    ax4.set_title('Selected Features by Type', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # ========================================
    # 4. FINAL FEATURE SELECTION DECISION
    # ========================================
    print("\n" + "=" * 60)
    print("FINAL FEATURE SELECTION DECISION")
    print("=" * 60)

    print("\nüéØ SELECTION STRATEGY JUSTIFICATION:")
    print("Using XGBoost-based selection as primary method because:")
    print("‚Ä¢ Tree-based methods handle non-linear relationships in startup data")
    print("‚Ä¢ XGBoost naturally handles mixed data types (binary, continuous)")
    print("‚Ä¢ Median threshold provides balanced feature reduction")
    print("‚Ä¢ Method aligns with ensemble modeling approach")

    # Use XGBoost selection as final
    selected_features_final = selected_features_xgb.tolist()

    print(f"\n‚úÖ FINAL SELECTED FEATURES ({len(selected_features_final)}):")
    for i, feature in enumerate(selected_features_final, 1):
        importance = xgb_model.feature_importances_[X_train.columns.get_loc(feature)]
        print(f"{i:2d}. {feature:<30} | Importance: {importance:.4f}")

    print(f"\nüìä FEATURE SELECTION SUMMARY:")
    print(f"‚Ä¢ Original features: {X_train.shape[1]}")
    print(f"‚Ä¢ Selected features: {len(selected_features_final)}")
    print(f"‚Ä¢ Reduction ratio: {(1 - len(selected_features_final)/X_train.shape[1]):.1%}")
    print(f"‚Ä¢ Features in common across all methods: {len(overlap_all)}")

    print("\nüèÜ DOMAIN EXPERTISE VALIDATION:")
    business_features = ['has_all_rounds', 'has_any', 'has_Investor', 'has_Seed', 'invalid_startup']
    selected_business = [f for f in business_features if f in selected_features_final]
    print(f"‚Ä¢ Business logic features selected: {len(selected_business)}/{len(business_features)}")
    print(f"‚Ä¢ Selected business features: {selected_business}")
    print("‚Ä¢ Validation: Domain-engineered features prove their predictive value")

    print("\n" + "=" * 60)
    print("FEATURE ENGINEERING & SELECTION COMPLETE")
    print("=" * 60)
    print("‚úÖ Ready for Model Building with optimized feature set")
    print("‚úÖ Feature importance validated through multiple methods")
    print("‚úÖ Business logic features confirmed as predictive")
    print("=" * 60)

    return df, selected_features_final, y

# Example usage:
# df_processed, selected_features, target = feature_engineering_selection(df, target='labels')

!pip install catboost

import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score
from sklearn.model_selection import cross_val_score, learning_curve, validation_curve, train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import time
import psutil
import os
from scipy import stats

def model_building(df, features, target, models_to_check=None):
    """
    Comprehensive model building and evaluation function for startup data

    Args:
        df (pd.DataFrame): Processed dataset with engineered features
        features (list): List of selected feature names
        target (pd.Series or str): Target variable or target column name
        models_to_check (dict, optional): Dictionary of models to evaluate
                                        {model_name: model_instance}
                                        If None, uses default model set

    Returns:
        dict: Dictionary containing all trained models and results
            - Keys are model names from models_to_check
            - Values are dictionaries with:
                * 'model': Trained model instance
                * 'roc_auc': ROC-AUC score
                * 'accuracy': Accuracy score
                * 'cv_mean': Cross-validation mean
                * 'cv_std': Cross-validation std
                * 'training_time': Time to train
                * 'inference_time': Time to predict
    """

    print("=== ENHANCED MODEL BUILDING ===")
    print("Following ML Project Guide - Model Building Section\n")

    # Handle target input
    if isinstance(target, str):
        if target not in df.columns:
            raise ValueError(f"Target column '{target}' not found in dataset")
        y = df[target]
        target_name = target
    else:
        y = target
        target_name = 'target'

    # Prepare features
    missing_features = [f for f in features if f not in df.columns]
    if missing_features:
        print(f"‚ö†Ô∏è Warning: Missing features in dataset: {missing_features}")
        available_features = [f for f in features if f in df.columns]
        print(f"Using {len(available_features)} available features")
        features = available_features

    X = df[features]

    print(f"üìä Dataset Summary:")
    print(f"‚Ä¢ Features: {len(features)}")
    print(f"‚Ä¢ Samples: {len(X)}")
    print(f"‚Ä¢ Target classes: {y.nunique()}")
    print(f"‚Ä¢ Class distribution: {y.value_counts().to_dict()}")

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )

    print(f"‚Ä¢ Training set: {len(X_train)} samples")
    print(f"‚Ä¢ Test set: {len(X_test)} samples")

    # ========================================
    # 1. BASELINE MODEL COMPARISONS
    # ========================================
    print("\n" + "=" * 60)
    print("BASELINE MODEL COMPARISONS")
    print("=" * 60)

    print("\nüéØ MODEL SELECTION STRATEGY:")
    print("Starting with simple baselines and building complexity systematically")
    print("‚Ä¢ Baseline 1: Logistic Regression (Linear model)")
    print("‚Ä¢ Baseline 2: Decision Tree (Simple non-linear)")
    print("‚Ä¢ Advanced 1: XGBoost (Gradient boosting)")
    print("‚Ä¢ Advanced 2: CatBoost (Category-optimized boosting)")
    print("‚Ä¢ Advanced 3: AdaBoost (Adaptive boosting)")

    # Use user-provided models or default set
    if models_to_check is None:
        # Default models with consistent approach
        models = {
            'Logistic Regression': LogisticRegression(
                random_state=42,
                max_iter=1000,
                class_weight=None  # Balanced dataset, no weighting needed
            ),

            'Decision Tree': DecisionTreeClassifier(
                random_state=42,
                max_depth=5,  # Prevent overfitting
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight=None  # Balanced dataset
            ),

            'XGBoost': XGBClassifier(
                # CONSISTENT PARAMETERS: Aligned with feature selection
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                subsample=0.8,
                colsample_bytree=0.8,
                # REMOVED INCORRECT CLASS WEIGHTS: Dataset is balanced
                use_label_encoder=False,
                eval_metric='logloss',
                random_state=42
            ),

            'CatBoost': CatBoostClassifier(
                iterations=100,
                learning_rate=0.1,
                depth=5,
                random_seed=42,
                verbose=False,
                class_weights=None  # Balanced dataset
            ),

            'AdaBoost': AdaBoostClassifier(
                n_estimators=100,
                learning_rate=0.1,
                random_state=42
            )
        }
        print(f"\nüîß USING DEFAULT MODEL SET:")
        print(f"‚Ä¢ {len(models)} default models loaded")
    else:
        # Validate user-provided models
        if not isinstance(models_to_check, dict):
            raise ValueError("models_to_check must be a dictionary with format {model_name: model_instance}")

        if len(models_to_check) == 0:
            raise ValueError("models_to_check dictionary cannot be empty")

        models = models_to_check.copy()  # Create copy to avoid modifying original
        print(f"\nüîß USING USER-PROVIDED MODEL SET:")
        print(f"‚Ä¢ {len(models)} custom models provided")

        # Validate each model has required methods
        for name, model in models.items():
            if not hasattr(model, 'fit'):
                raise ValueError(f"Model '{name}' must have a 'fit' method")
            if not hasattr(model, 'predict'):
                raise ValueError(f"Model '{name}' must have a 'predict' method")
            if not hasattr(model, 'predict_proba'):
                raise ValueError(f"Model '{name}' must have a 'predict_proba' method")

        print(f"‚Ä¢ All models validated successfully")

    print(f"‚Ä¢ Models to evaluate: {list(models.keys())}")

    print(f"\nüîß HYPERPARAMETER INFO:")
    if models_to_check is None:
        print("‚Ä¢ Default XGBoost parameters CONSISTENT with feature selection phase")
        print("‚Ä¢ REMOVED class_weight corrections (dataset is balanced)")
        print("‚Ä¢ All default models use same random_state=42 for reproducibility")
    else:
        print("‚Ä¢ Using user-provided model configurations")
        print("‚Ä¢ Ensure your models have appropriate hyperparameters for your dataset")

    # ========================================
    # 2. ENHANCED EVALUATION FRAMEWORK
    # ========================================
    print("\n" + "=" * 60)
    print("COMPREHENSIVE MODEL EVALUATION")
    print("=" * 60)

    # Storage for results
    model_results = {}
    training_times = {}
    inference_times = {}
    model_sizes = {}

    print("\nüèÉ‚Äç‚ôÇÔ∏è TRAINING AND EVALUATING MODELS...")

    for name, model in models.items():
        print(f"\nüìä Training {name}...")

        # Measure training time
        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time
        training_times[name] = training_time

        # Measure inference time
        start_time = time.time()
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        inference_time = time.time() - start_time
        inference_times[name] = inference_time

        # Measure model size (approximate)
        model_size = len(str(model)) / 1024  # Rough approximation in KB
        model_sizes[name] = model_size

        # Cross-validation scores
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')

        # Performance metrics
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        accuracy = accuracy_score(y_test, y_pred)

        # Store results
        model_results[name] = {
            'model': model,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'roc_auc': roc_auc,
            'accuracy': accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'training_time': training_time,
            'inference_time': inference_time,
            'model_size': model_size
        }

        print(f"   ‚úÖ ROC-AUC: {roc_auc:.4f} | Accuracy: {accuracy:.4f} | CV: {cv_scores.mean():.4f}¬±{cv_scores.std():.4f}")

    # ========================================
    # 3. COMPREHENSIVE RESULTS ANALYSIS
    # ========================================
    print("\n" + "=" * 60)
    print("COMPREHENSIVE RESULTS ANALYSIS")
    print("=" * 60)

    # Create results summary
    results_df = pd.DataFrame({
        'Model': list(model_results.keys()),
        'ROC_AUC': [model_results[name]['roc_auc'] for name in model_results.keys()],
        'Accuracy': [model_results[name]['accuracy'] for name in model_results.keys()],
        'CV_Mean': [model_results[name]['cv_mean'] for name in model_results.keys()],
        'CV_Std': [model_results[name]['cv_std'] for name in model_results.keys()],
        'Training_Time': [model_results[name]['training_time'] for name in model_results.keys()],
        'Inference_Time': [model_results[name]['inference_time'] for name in model_results.keys()],
        'Model_Size_KB': [model_results[name]['model_size'] for name in model_results.keys()]
    })

    # Sort by ROC-AUC
    results_df = results_df.sort_values('ROC_AUC', ascending=False)

    print("\nüìä MODEL PERFORMANCE SUMMARY:")
    print(results_df.round(4).to_string(index=False))

    # ========================================
    # 4. VISUALIZATION
    # ========================================
    print("\n" + "=" * 60)
    print("MODEL PERFORMANCE VISUALIZATION")
    print("=" * 60)

    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Comprehensive Model Evaluation Dashboard', fontsize=16, fontweight='bold')

    # Plot 1: ROC-AUC Comparison
    ax1 = axes[0, 0]
    bars1 = ax1.bar(results_df['Model'], results_df['ROC_AUC'],
                    color='skyblue', edgecolor='navy', alpha=0.7)
    ax1.set_ylabel('ROC-AUC Score', fontweight='bold')
    ax1.set_title('ROC-AUC Performance', fontweight='bold')
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(axis='y', alpha=0.3)

    # Highlight best model
    best_idx = results_df['ROC_AUC'].idxmax()
    bars1[0].set_color('gold')
    bars1[0].set_alpha(1.0)

    # Plot 2: Cross-Validation Scores with Error Bars
    ax2 = axes[0, 1]
    ax2.bar(results_df['Model'], results_df['CV_Mean'],
            yerr=results_df['CV_Std'], capsize=5,
            color='lightcoral', edgecolor='darkred', alpha=0.7)
    ax2.set_ylabel('CV ROC-AUC', fontweight='bold')
    ax2.set_title('Cross-Validation Performance', fontweight='bold')
    ax2.tick_params(axis='x', rotation=45)
    ax2.grid(axis='y', alpha=0.3)

    # Plot 3: Training Time Comparison
    ax3 = axes[0, 2]
    bars3 = ax3.bar(results_df['Model'], results_df['Training_Time'],
                    color='lightgreen', edgecolor='darkgreen', alpha=0.7)
    ax3.set_ylabel('Training Time (seconds)', fontweight='bold')
    ax3.set_title('Training Efficiency', fontweight='bold')
    ax3.tick_params(axis='x', rotation=45)
    ax3.grid(axis='y', alpha=0.3)

    # Plot 4: ROC Curves
    ax4 = axes[1, 0]
    colors = ['blue', 'red', 'green', 'purple', 'orange']
    for i, (name, result) in enumerate(model_results.items()):
        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
        ax4.plot(fpr, tpr, color=colors[i], label=f"{name} (AUC={result['roc_auc']:.3f})")

    ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    ax4.set_xlabel('False Positive Rate', fontweight='bold')
    ax4.set_ylabel('True Positive Rate', fontweight='bold')
    ax4.set_title('ROC Curves', fontweight='bold')
    ax4.legend(loc='lower right')
    ax4.grid(alpha=0.3)

    # Plot 5: Performance vs Efficiency Scatter
    ax5 = axes[1, 1]
    scatter = ax5.scatter(results_df['Training_Time'], results_df['ROC_AUC'],
                         s=results_df['Model_Size_KB']*10, alpha=0.7,
                         c=range(len(results_df)), cmap='viridis')

    for i, model in enumerate(results_df['Model']):
        ax5.annotate(model, (results_df.iloc[i]['Training_Time'], results_df.iloc[i]['ROC_AUC']),
                    xytext=(5, 5), textcoords='offset points', fontsize=9)

    ax5.set_xlabel('Training Time (seconds)', fontweight='bold')
    ax5.set_ylabel('ROC-AUC Score', fontweight='bold')
    ax5.set_title('Performance vs Efficiency\n(Size = Model Complexity)', fontweight='bold')
    ax5.grid(alpha=0.3)

    # Plot 6: Model Comparison Radar Chart
    ax6 = axes[1, 2]

    # Normalize metrics for radar chart
    metrics = ['ROC_AUC', 'Accuracy', 'CV_Mean']
    normalized_results = results_df[metrics].copy()
    for col in metrics:
        normalized_results[col] = (normalized_results[col] - normalized_results[col].min()) / \
                                 (normalized_results[col].max() - normalized_results[col].min())

    # Create a simple bar chart instead of radar (easier to implement)
    x_pos = np.arange(len(metrics))
    width = 0.15

    for i, model in enumerate(results_df['Model'][:3]):  # Top 3 models
        offset = (i - 1) * width
        values = normalized_results.iloc[i][metrics].values
        ax6.bar(x_pos + offset, values, width, label=model, alpha=0.7)

    ax6.set_xlabel('Metrics', fontweight='bold')
    ax6.set_ylabel('Normalized Score', fontweight='bold')
    ax6.set_title('Top 3 Models Comparison', fontweight='bold')
    ax6.set_xticks(x_pos)
    ax6.set_xticklabels(metrics)
    ax6.legend()
    ax6.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.show()

    # ========================================
    # 5. RETURN ALL TRAINED MODELS
    # ========================================
    print("\n" + "=" * 60)
    print("RETURNING ALL TRAINED MODELS")
    print("=" * 60)

    # Create simplified return dictionary with all trained models
    trained_models = {}

    for name in models.keys():
        trained_models[name] = {
            'model': model_results[name]['model'],
            'roc_auc': model_results[name]['roc_auc'],
            'accuracy': model_results[name]['accuracy'],
            'cv_mean': model_results[name]['cv_mean'],
            'cv_std': model_results[name]['cv_std'],
            'training_time': model_results[name]['training_time'],
            'inference_time': model_results[name]['inference_time']
        }

    # Select best model based on ROC-AUC for reporting
    best_model_name = results_df.iloc[0]['Model']
    best_model = model_results[best_model_name]['model']
    best_roc_auc = results_df.iloc[0]['ROC_AUC']

    print(f"\nüèÜ BEST PERFORMING MODEL: {best_model_name}")
    print(f"‚Ä¢ ROC-AUC Score: {best_roc_auc:.4f}")
    print(f"‚Ä¢ Accuracy: {model_results[best_model_name]['accuracy']:.4f}")
    print(f"‚Ä¢ Cross-Validation: {model_results[best_model_name]['cv_mean']:.4f} ¬± {model_results[best_model_name]['cv_std']:.4f}")
    print(f"‚Ä¢ Training Time: {model_results[best_model_name]['training_time']:.2f} seconds")

    # Detailed classification report for best model
    print(f"\nüìã DETAILED CLASSIFICATION REPORT - {best_model_name}:")
    y_pred_best = model_results[best_model_name]['y_pred']
    print(classification_report(y_test, y_pred_best))

    # Confusion Matrix
    print(f"\nüîç CONFUSION MATRIX - {best_model_name}:")
    cm = confusion_matrix(y_test, y_pred_best)
    print(cm)

    print("\n" + "=" * 60)
    print("MODEL BUILDING COMPLETE")
    print("=" * 60)
    print(f"‚úÖ Returning {len(trained_models)} trained models")
    print(f"‚úÖ Best performing model: {best_model_name} (ROC-AUC: {best_roc_auc:.4f})")
    print("‚úÖ All models ready for use")
    print("=" * 60)

    return trained_models



import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import (classification_report, precision_recall_curve, make_scorer,
                           precision_score, recall_score, f1_score as f1_metric, roc_auc_score)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import time

def custom_hyperparameter_optimizer(models_dictionary, X, y, evaluation_method='precision', business_constraints=None):
    """
    Custom hyperparameter optimizer with built-in parameter grids and business constraints

    Args:
        models_dictionary (dict): Dictionary of models to optimize {model_name: model_instance}
        X (array-like): Training features
        y (array-like): Training target
        evaluation_method (str): Primary optimization metric ('precision', 'recall', 'f1', 'roc_auc')
        business_constraints (dict): Business constraints with keys:
            - 'min_precision': Minimum required precision
            - 'min_recall': Minimum required recall
            - 'min_f1': Minimum required F1-score

    Returns:
        dict: Optimized models with results
            - Keys: Model names from models_dictionary
            - Values: Dict with 'optimized_model', 'best_params', 'best_scores', 'optimization_results'
    """

    print("=== CUSTOM HYPERPARAMETER OPTIMIZER ===")
    print("Following ML Project Guide - Parameter Tuning Section\n")

    # Default business constraints
    if business_constraints is None:
        business_constraints = {
            'min_precision': 0.70,
            'min_recall': 0.70,
            'min_f1': 0.70
        }

    print("=" * 60)
    print("BUSINESS CONSTRAINT DEFINITION")
    print("=" * 60)

    print(f"\nüéØ OPTIMIZATION CONFIGURATION:")
    print(f"‚Ä¢ Primary optimization metric: {evaluation_method}")
    print(f"‚Ä¢ Models to optimize: {len(models_dictionary)}")
    print(f"‚Ä¢ Training samples: {len(X)}")

    print(f"\nüìã BUSINESS CONSTRAINTS:")
    for key, value in business_constraints.items():
        print(f"‚Ä¢ {key}: {value}")


    # Built-in parameter grids for different models
    PARAMETER_GRIDS = {
        'LogisticRegression': {
            'C': [0.1, 10.0, 100.0],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga'],
            'max_iter': [1000],
            'random_state': [42]
        },

        'DecisionTreeClassifier': {
            'max_depth': [ 7, 10],
            'min_samples_split': [ 5, 10],
            'min_samples_leaf': [ 5, 10],
            'criterion': ['gini', 'entropy'],
            'max_features': ['sqrt', 'log2'],
            'random_state': [42]
        },

        'AdaBoostClassifier': {
            'n_estimators': [50, 100],
            'learning_rate': [0.1, 0.5, 1.0],
            'algorithm': ['SAMME'],
            'random_state': [42]
          },

        'CatBoostClassifier': {
            'iterations': [100, 50],
            'learning_rate': [ 0.1, 0.2],
            'depth': [4, 6],
            'l2_leaf_reg': [1,  5],
            'random_seed': [42],
            'verbose': [False]
        },

        'XGBClassifier':  {
            'n_estimators': [100, 50],
            'learning_rate': [0.1, 0.2],
            'max_depth': [3, 5],
            'subsample': [0.8, 0.9],
            'colsample_bytree': [0.8, 0.9],
            'random_state': [42],
            'use_label_encoder': [False],
            'eval_metric': ['logloss']
        }
    }

    print(f"\nüîß BUILT-IN PARAMETER GRIDS:")
    for model_class, params in PARAMETER_GRIDS.items():
        param_combinations = np.prod([len(v) for v in params.values()])
        print(f"‚Ä¢ {model_class}: {param_combinations} combinations")

    def get_model_class_name(model):
        """Get the class name of a model instance"""
        return model.__class__.__name__

    def custom_constraint_optimizer(model, param_grid, X, y, min_precision, min_recall, min_f1, scoring, cv=5):
        """
        Custom optimizer with business constraints for a single model
        """

        print(f"\nüîç OPTIMIZING {get_model_class_name(model)}:")
        print(f"‚Ä¢ Parameter combinations: {np.prod([len(v) for v in param_grid.values()])}")
        print(f"‚Ä¢ Constraints: Precision‚â•{min_precision}, Recall‚â•{min_recall}, F1‚â•{min_f1}")

        # Storage for results
        valid_results = []
        all_results = []

        # Create custom scorer that penalizes constraint violations
        def constraint_aware_scorer(estimator, X, y):
            y_pred = estimator.predict(X)
            y_pred_proba = estimator.predict_proba(X)[:, 1]

            precision = precision_score(y, y_pred, zero_division=0)
            recall = recall_score(y, y_pred, zero_division=0)
            f1 = f1_metric(y, y_pred, zero_division=0)
            roc_auc = roc_auc_score(y, y_pred_proba)

            # Check constraints
            meets_precision = precision >= min_precision
            meets_recall = recall >= min_recall
            meets_f1 = f1 >= min_f1

            # Heavy penalty if constraints not met
            if not (meets_precision and meets_recall and meets_f1):
                penalty = -10
            else:
                penalty = 0

            # Primary score based on chosen metric
            if scoring == 'precision':
                primary_score = precision
            elif scoring == 'recall':
                primary_score = recall
            elif scoring == 'f1':
                primary_score = f1
            else:  # roc_auc
                primary_score = roc_auc

            return primary_score + penalty

        # Custom scorer for sklearn
        custom_scorer = make_scorer(constraint_aware_scorer, greater_is_better=True)

        start_time = time.time()

        # Perform grid search
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grid,
            scoring=custom_scorer,
            cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),
            n_jobs=-1,
            verbose=0
        )

        grid_search.fit(X, y)

        # Detailed evaluation of all parameter combinations
        for params in grid_search.cv_results_['params']:
            # Create temporary model with these parameters
            temp_model = model.__class__(**params)

            # Get cross-validation scores for all metrics
            try:
                cv_precision = cross_val_score(temp_model, X, y, cv=cv, scoring='precision').mean()
                cv_recall = cross_val_score(temp_model, X, y, cv=cv, scoring='recall').mean()
                cv_f1 = cross_val_score(temp_model, X, y, cv=cv, scoring='f1').mean()
                cv_roc_auc = cross_val_score(temp_model, X, y, cv=cv, scoring='roc_auc').mean()
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error evaluating params {params}: {e}")
                continue

            # Check if constraints are met
            meets_constraints = (cv_precision >= min_precision and
                               cv_recall >= min_recall and
                               cv_f1 >= min_f1)

            result = {
                'params': params,
                'precision': cv_precision,
                'recall': cv_recall,
                'f1': cv_f1,
                'roc_auc': cv_roc_auc,
                'meets_constraints': meets_constraints,
                'constraint_score': cv_precision + cv_recall + cv_f1 if meets_constraints else 0
            }

            all_results.append(result)

            if meets_constraints:
                valid_results.append(result)

        optimization_time = time.time() - start_time

        # Select best valid result
        if valid_results:
            if scoring == 'precision':
                best_result = max(valid_results, key=lambda x: x['precision'])
            elif scoring == 'recall':
                best_result = max(valid_results, key=lambda x: x['recall'])
            elif scoring == 'f1':
                best_result = max(valid_results, key=lambda x: x['f1'])
            else:
                best_result = max(valid_results, key=lambda x: x['roc_auc'])
        else:
            # No valid results - return best overall but flag constraint violation
            if all_results:
                best_result = max(all_results, key=lambda x: x['f1'])
                best_result['constraint_violation'] = True
            else:
                # Fallback - use grid search best result
                best_result = {
                    'params': grid_search.best_params_,
                    'precision': 0,
                    'recall': 0,
                    'f1': 0,
                    'roc_auc': 0,
                    'meets_constraints': False,
                    'constraint_violation': True
                }

        print(f"   ‚úÖ Complete: {optimization_time:.2f}s | Valid combinations: {len(valid_results)}/{len(all_results)}")
        if valid_results:
            print(f"   üìä Best: Precision={best_result['precision']:.3f}, Recall={best_result['recall']:.3f}, F1={best_result['f1']:.3f}")
        else:
            print(f"   ‚ö†Ô∏è No combinations meet all constraints")

        return {
            'best_params': best_result['params'],
            'best_result': best_result,
            'all_results': all_results,
            'valid_results': valid_results,
            'optimization_time': optimization_time,
            'constraints_met': len(valid_results) > 0
        }

    # ========================================
    # MAIN OPTIMIZATION LOOP
    # ========================================
    print("\n" + "=" * 60)
    print("RUNNING OPTIMIZATION FOR ALL MODELS")
    print("=" * 60)

    optimization_results = {}
    optimized_models = {}

    for model_name, model in models_dictionary.items():
        print(f"\n{'='*20} OPTIMIZING {model_name.upper()} {'='*20}")

        model_class_name = get_model_class_name(model)

        # Get parameter grid for this model type
        if model_class_name in PARAMETER_GRIDS:
            param_grid = PARAMETER_GRIDS[model_class_name]
        else:
            print(f"‚ö†Ô∏è No parameter grid found for {model_class_name}. Using basic grid.")
            # Basic fallback grid
            param_grid = {'random_state': [42]} if hasattr(model, 'random_state') else {}

        if not param_grid or len(param_grid) == 0:
            print(f"‚ö†Ô∏è Empty parameter grid for {model_name}. Skipping optimization.")
            # Use original model
            optimized_models[model_name] = {
                'optimized_model': model,
                'best_params': {},
                'best_scores': {'precision': 0, 'recall': 0, 'f1': 0, 'roc_auc': 0},
                'optimization_results': {'constraints_met': False, 'optimization_time': 0}
            }
            continue

        # Run optimization
        result = custom_constraint_optimizer(
            model=model,
            param_grid=param_grid,
            X=X,
            y=y,
            min_precision=business_constraints.get('min_precision', 0.7),
            min_recall=business_constraints.get('min_recall', 0.7),
            min_f1=business_constraints.get('min_f1', 0.7),
            scoring=evaluation_method,
            cv=5
        )

        optimization_results[model_name] = result

        # Create optimized model with best parameters
        try:
            optimized_model = model.__class__(**result['best_params'])
            optimized_model.fit(X, y)

            optimized_models[model_name] = {
                'optimized_model': optimized_model,
                'best_params': result['best_params'],
                'best_scores': {
                    'precision': result['best_result']['precision'],
                    'recall': result['best_result']['recall'],
                    'f1': result['best_result']['f1'],
                    'roc_auc': result['best_result']['roc_auc']
                },
                'optimization_results': result
            }
        except Exception as e:
            print(f"   ‚ùå Error creating optimized model: {e}")
            # Fallback to original model
            optimized_models[model_name] = {
                'optimized_model': model,
                'best_params': {},
                'best_scores': {'precision': 0, 'recall': 0, 'f1': 0, 'roc_auc': 0},
                'optimization_results': result
            }

    # ========================================
    # RESULTS SUMMARY
    # ========================================
    print("\n" + "=" * 60)
    print("OPTIMIZATION RESULTS SUMMARY")
    print("=" * 60)

    # Create results comparison
    comparison_data = []
    for model_name in optimized_models.keys():
        if model_name in optimization_results:
            result = optimization_results[model_name]
            best = result['best_result']
            comparison_data.append({
                'Model': model_name,
                'Precision': f"{best['precision']:.4f}",
                'Recall': f"{best['recall']:.4f}",
                'F1-Score': f"{best['f1']:.4f}",
                'ROC-AUC': f"{best['roc_auc']:.4f}",
                'Constraints Met': '‚úÖ' if result['constraints_met'] else '‚ùå',
                'Valid Combos': f"{len(result['valid_results'])}/{len(result['all_results'])}",
                'Time (s)': f"{result['optimization_time']:.1f}"
            })

    if comparison_data:
        results_df = pd.DataFrame(comparison_data)
        print("\nüìä OPTIMIZATION RESULTS:")
        print(results_df.to_string(index=False))

        # Find best model meeting constraints
        valid_models = {name: result for name, result in optimization_results.items()
                       if result['constraints_met']}

        if valid_models:
            best_model_name = max(valid_models.keys(),
                                key=lambda x: valid_models[x]['best_result'][evaluation_method])

            print(f"\nüèÜ BEST MODEL MEETING CONSTRAINTS: {best_model_name}")
            print(f"‚Ä¢ Precision: {valid_models[best_model_name]['best_result']['precision']:.4f}")
            print(f"‚Ä¢ Recall: {valid_models[best_model_name]['best_result']['recall']:.4f}")
            print(f"‚Ä¢ F1-Score: {valid_models[best_model_name]['best_result']['f1']:.4f}")
            print(f"‚Ä¢ ROC-AUC: {valid_models[best_model_name]['best_result']['roc_auc']:.4f}")
        else:
            print("\n‚ö†Ô∏è NO MODELS MEET ALL BUSINESS CONSTRAINTS")
            print("Consider relaxing constraints or improving data quality")

    total_time = sum(result['optimization_time'] for result in optimization_results.values())
    print(f"\n‚ö° OPTIMIZATION SUMMARY:")
    print(f"‚Ä¢ Total time: {total_time:.1f} seconds")
    print(f"‚Ä¢ Models optimized: {len(optimized_models)}")
    print(f"‚Ä¢ Models meeting constraints: {len([r for r in optimization_results.values() if r['constraints_met']])}")

    print("\n" + "=" * 60)
    print("HYPERPARAMETER OPTIMIZATION COMPLETE")
    print("=" * 60)

    return optimized_models

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                           precision_score, recall_score, f1_score, accuracy_score)
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
import time
warnings.filterwarnings('ignore')

# Import all our custom functions
# (Assuming the functions are already defined in the environment)

class StartupMLPipeline:
    """
    Complete Machine Learning Pipeline for Startup Success Prediction

    This pipeline integrates all custom functions:
    1. Data Preprocessing
    2. Feature Engineering & Selection
    3. Model Building & Evaluation
    4. Hyperparameter Optimization
    """

    def __init__(self, target_column='labels', random_state=42):
        """
        Initialize the pipeline

        Args:
            target_column (str): Name of target column
            random_state (int): Random state for reproducibility
        """
        self.target_column = target_column
        self.random_state = random_state
        self.pipeline_results = {}

        print("üöÄ STARTUP ML PIPELINE INITIALIZED")
        print("="*60)
        print("üìã Pipeline Components:")
        print("‚Ä¢ Stage 1: Data Preprocessing")
        print("‚Ä¢ Stage 2: Feature Engineering & Selection")
        print("‚Ä¢ Stage 3: Model Building & Evaluation")
        print("‚Ä¢ Stage 4: Hyperparameter Optimization")
        print("="*60)

    def run_complete_pipeline(self, raw_data_path=None, raw_df=None,
                            custom_models=None, business_constraints=None,
                            evaluation_method='precision', skip_optimization=False):
        """
        Run the complete ML pipeline from raw data to optimized models

        Args:
            raw_data_path (str): Path to raw CSV data file
            raw_df (pd.DataFrame): Raw DataFrame (alternative to file path)
            custom_models (dict): Custom models to evaluate {name: model_instance}
            business_constraints (dict): Business constraints for optimization
            evaluation_method (str): Primary optimization metric
            skip_optimization (bool): Skip hyperparameter optimization stage

        Returns:
            dict: Complete pipeline results
        """

        pipeline_start_time = time.time()

        print("\nüé¨ STARTING COMPLETE ML PIPELINE")
        print("="*80)

        # ========================================
        # STAGE 0: DATA LOADING
        # ========================================
        print("\nüìÅ STAGE 0: DATA LOADING")
        print("-"*40)

        if raw_df is not None:
            df_raw = raw_df.copy()
            print("‚úÖ DataFrame provided directly")
        elif raw_data_path is not None:
            df_raw = pd.read_csv(raw_data_path)
            print(f"‚úÖ Data loaded from: {raw_data_path}")
        else:
            raise ValueError("Either raw_data_path or raw_df must be provided")

        print(f"üìä Raw data shape: {df_raw.shape}")
        print(f"üìä Columns: {list(df_raw.columns)}")

        self.pipeline_results['raw_data_shape'] = df_raw.shape
        self.pipeline_results['stage_0_time'] = time.time() - pipeline_start_time

        # ========================================
        # STAGE 1: DATA PREPROCESSING
        # ========================================
        stage1_start = time.time()
        print("\nüßπ STAGE 1: DATA PREPROCESSING")
        print("-"*40)

        try:
            df_preprocessed = preprocessing(df_raw)
            print("‚úÖ Preprocessing completed successfully")

            self.pipeline_results['preprocessed_data_shape'] = df_preprocessed.shape
            self.pipeline_results['stage_1_time'] = time.time() - stage1_start
            self.pipeline_results['preprocessing_success'] = True

        except Exception as e:
            print(f"‚ùå Preprocessing failed: {e}")
            self.pipeline_results['preprocessing_success'] = False
            self.pipeline_results['preprocessing_error'] = str(e)
            return self.pipeline_results

        # ========================================
        # STAGE 2: FEATURE ENGINEERING & SELECTION
        # ========================================
        stage2_start = time.time()
        print("\nüîß STAGE 2: FEATURE ENGINEERING & SELECTION")
        print("-"*40)

        try:
            df_engineered, selected_features, target = feature_engineering_selection(
                df_preprocessed, target=self.target_column
            )
            print("‚úÖ Feature engineering and selection completed")
            print(f"üìä Selected features: {len(selected_features)}")

            self.pipeline_results['engineered_data_shape'] = df_engineered.shape
            self.pipeline_results['selected_features'] = selected_features
            self.pipeline_results['n_selected_features'] = len(selected_features)
            self.pipeline_results['stage_2_time'] = time.time() - stage2_start
            self.pipeline_results['feature_engineering_success'] = True

        except Exception as e:
            print(f"‚ùå Feature engineering failed: {e}")
            self.pipeline_results['feature_engineering_success'] = False
            self.pipeline_results['feature_engineering_error'] = str(e)
            return self.pipeline_results

        # ========================================
        # STAGE 3: MODEL BUILDING & EVALUATION
        # ========================================
        stage3_start = time.time()
        print("\nü§ñ STAGE 3: MODEL BUILDING & EVALUATION")
        print("-"*40)

        # Default models if none provided
        if custom_models is None:
            custom_models = {
                'Logistic_Regression': LogisticRegression(random_state=self.random_state, max_iter=1000),
                'Decision_Tree': DecisionTreeClassifier(random_state=self.random_state, max_depth=5),
                'XGBoost': XGBClassifier(random_state=self.random_state, use_label_encoder=False, eval_metric='logloss'),
                'CatBoost': CatBoostClassifier(random_seed=self.random_state, verbose=False),
                'AdaBoost': AdaBoostClassifier(random_state=self.random_state)
            }

        try:
            baseline_models = model_building(
                df_engineered, selected_features, target, models_to_check=custom_models
            )
            print("‚úÖ Model building and evaluation completed")
            print(f"üìä Models evaluated: {len(baseline_models)}")

            # Find best baseline model
            best_baseline_model = None
            best_baseline_score = 0
            best_baseline_name = None

            for model_name, model_info in baseline_models.items():
                if evaluation_method in model_info:
                    score = model_info[evaluation_method]
                    if score > best_baseline_score:
                        best_baseline_score = score
                        best_baseline_model = model_info['model']
                        best_baseline_name = model_name

            self.pipeline_results['baseline_models'] = baseline_models
            self.pipeline_results['best_baseline_model'] = best_baseline_name
            self.pipeline_results['best_baseline_score'] = best_baseline_score
            self.pipeline_results['stage_3_time'] = time.time() - stage3_start
            self.pipeline_results['model_building_success'] = True

        except Exception as e:
            print(f"‚ùå Model building failed: {e}")
            self.pipeline_results['model_building_success'] = False
            self.pipeline_results['model_building_error'] = str(e)
            return self.pipeline_results

        # ========================================
        # STAGE 4: HYPERPARAMETER OPTIMIZATION
        # ========================================
        if not skip_optimization:
            stage4_start = time.time()
            print("\n‚ö° STAGE 4: HYPERPARAMETER OPTIMIZATION")
            print("-"*40)

            # Default business constraints
            if business_constraints is None:
                business_constraints = {
                    'min_precision': 0.75,
                    'min_recall': 0.70,
                    'min_f1': 0.72
                }

            # Prepare data for optimization
            X = df_engineered[selected_features]
            y = target

            try:
                optimized_models = custom_hyperparameter_optimizer(
                    models_dictionary=custom_models,
                    X=X,
                    y=y,
                    evaluation_method=evaluation_method,
                    business_constraints=business_constraints
                )
                print("‚úÖ Hyperparameter optimization completed")
                print(f"üìä Models optimized: {len(optimized_models)}")

                # Find best optimized model
                best_optimized_model = None
                best_optimized_score = 0
                best_optimized_name = None


                for model_name, model_info in optimized_models.items():
                    if evaluation_method in model_info['best_scores']:
                        score = model_info['best_scores'][evaluation_method]
                        if score > best_optimized_score:
                            best_optimized_score = score
                            best_optimized_model = model_info['optimized_model']
                            best_optimized_name = model_name
                # üîß  PRINT BEST HYPER-PARAMETERS (no CV)
                for mdl_name, mdl_info in optimized_models.items():
                    best_params = mdl_info.get('best_params', {})
                    print(f"\nüîß Optimal hyper-parameters for {mdl_name}:")
                    for k, v in best_params.items():
                        print(f"   ‚Ä¢ {k}: {v}")


                self.pipeline_results['optimized_models'] = optimized_models
                self.pipeline_results['best_optimized_model'] = best_optimized_name
                self.pipeline_results['best_optimized_score'] = best_optimized_score
                self.pipeline_results['stage_4_time'] = time.time() - stage4_start
                self.pipeline_results['optimization_success'] = True

            except Exception as e:
                print(f"‚ùå Hyperparameter optimization failed: {e}")
                self.pipeline_results['optimization_success'] = False
                self.pipeline_results['optimization_error'] = str(e)
        else:
            print("\n‚è≠ STAGE 4: HYPERPARAMETER OPTIMIZATION SKIPPED")
            self.pipeline_results['optimization_success'] = False
            self.pipeline_results['optimization_skipped'] = True




        # ========================================
        # PIPELINE COMPLETION
        # ========================================
        total_pipeline_time = time.time() - pipeline_start_time

        print("\nüéâ PIPELINE COMPLETION SUMMARY")
        print("="*80)
        print(f"‚úÖ Total pipeline time: {total_pipeline_time:.2f} seconds")
        print(f"üìä Data: {df_raw.shape[0]} samples ‚Üí {len(selected_features)} features")

        if self.pipeline_results.get('model_building_success', False):
            print(f"üèÜ Best baseline model: {best_baseline_name} ({evaluation_method}: {best_baseline_score:.4f})")

        if self.pipeline_results.get('optimization_success', False):
            print(f"‚ö° Best optimized model: {best_optimized_name} ({evaluation_method}: {best_optimized_score:.4f})")
            improvement = best_optimized_score - best_baseline_score
            print(f"üìà Optimization improvement: {improvement:.4f} ({improvement/best_baseline_score*100:.1f}%)")

        self.pipeline_results['total_pipeline_time'] = total_pipeline_time
        self.pipeline_results['pipeline_completed'] = True

        print("="*80)
        return self.pipeline_results

    def get_best_model(self):
        """Get the best performing model from the pipeline"""
        if self.pipeline_results.get('optimization_success', False):
            best_name = self.pipeline_results['best_optimized_model']
            return self.pipeline_results['optimized_models'][best_name]['optimized_model']
        elif self.pipeline_results.get('model_building_success', False):
            best_name = self.pipeline_results['best_baseline_model']
            return self.pipeline_results['baseline_models'][best_name]['model']
        else:
            return None

    def get_pipeline_summary(self):
        """Get a comprehensive summary of the pipeline results"""
        if not self.pipeline_results.get('pipeline_completed', False):
            return "Pipeline not completed yet. Run run_complete_pipeline() first."

        summary = {
            'Data Processing': {
                'Raw data shape': self.pipeline_results.get('raw_data_shape', 'N/A'),
                'Preprocessed shape': self.pipeline_results.get('preprocessed_data_shape', 'N/A'),
                'Selected features': self.pipeline_results.get('n_selected_features', 'N/A')
            },
            'Model Performance': {
                'Best baseline model': self.pipeline_results.get('best_baseline_model', 'N/A'),
                'Best baseline score': self.pipeline_results.get('best_baseline_score', 'N/A'),
                'Best optimized model': self.pipeline_results.get('best_optimized_model', 'N/A'),
                'Best optimized score': self.pipeline_results.get('best_optimized_score', 'N/A')
            },
            'Timing': {
                'Preprocessing time': f"{self.pipeline_results.get('stage_1_time', 0):.2f}s",
                'Feature engineering time': f"{self.pipeline_results.get('stage_2_time', 0):.2f}s",
                'Model building time': f"{self.pipeline_results.get('stage_3_time', 0):.2f}s",
                'Optimization time': f"{self.pipeline_results.get('stage_4_time', 0):.2f}s",
                'Total pipeline time': f"{self.pipeline_results.get('total_pipeline_time', 0):.2f}s"
            }
        }

        return summary

    def save_results(self, filepath):
        """Save pipeline results to a file"""
        try:
            # Create a serializable version of results
            save_data = {
                key: value for key, value in self.pipeline_results.items()
                if not key.endswith('_models')  # Exclude model objects
            }

            import json
            with open(filepath, 'w') as f:
                json.dump(save_data, f, indent=2, default=str)

            print(f"‚úÖ Pipeline results saved to: {filepath}")
        except Exception as e:
            print(f"‚ùå Failed to save results: {e}")

# ========================================
# PIPELINE USAGE EXAMPLES
# ========================================

def run_startup_prediction_pipeline(data_path=None, df=None):
    """
    Convenience function to run the complete startup prediction pipeline

    Args:
        data_path (str): Path to startup data CSV
        df (pd.DataFrame): Startup data DataFrame

    Returns:
        tuple: (pipeline_instance, best_model, summary)
    """

    # Initialize pipeline
    pipeline = StartupMLPipeline(target_column='labels', random_state=42)

    # Define business constraints for startup prediction
    startup_constraints = {
        'min_precision': 0.80,  # High precision to avoid false positive investments
        'min_recall': 0.70,     # Good recall to catch successful startups
        'min_f1': 0.75          # Balanced F1 score
    }

    # Define models to evaluate
    startup_models = {
        'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Decision_Tree': DecisionTreeClassifier(random_state=42, max_depth=7),
        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
        'CatBoost': CatBoostClassifier(random_seed=42, verbose=False),
        'AdaBoost': AdaBoostClassifier(random_state=42)
    }

    # Run complete pipeline
    results = pipeline.run_complete_pipeline(
        raw_data_path=data_path,
        raw_df=df,
        custom_models=startup_models,
        business_constraints=startup_constraints,
        evaluation_method='precision',  # Optimize for precision in startup investing
        skip_optimization=False
    )

    # Get best model and summary
    best_model = pipeline.get_best_model()
    summary = pipeline.get_pipeline_summary()

    return pipeline, best_model, summary



print("\nüéØ COMPLETE ML PIPELINE READY")
print("="*60)
print("üìã Available Functions:")
print("‚Ä¢ StartupMLPipeline class - Complete pipeline management")
print("‚Ä¢ run_startup_prediction_pipeline() - Convenience function")
print("‚Ä¢ All individual functions: preprocessing, feature_engineering_selection,")
print("  model_building, custom_hyperparameter_optimizer")
print("="*60)

pipeline, best_model, summary = run_startup_prediction_pipeline(data_path='/content/startup data.csv')

# ==============================================================
# A)  BUILD X , y  (reuse the same helpers from the pipeline)
# ==============================================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
from scipy.stats import ttest_rel
from itertools import combinations


df_raw = pd.read_csv("/content/startup data.csv")
df_prep = preprocessing(df_raw)
df_eng, selected_features, target = feature_engineering_selection(
    df_prep, target="labels")

X = df_eng[selected_features].values
y = target.values

# ==============================================================
# B)  DEFINE OPTIMISED MODELS
# ==============================================================
models = {
    "Logistic Regression": LogisticRegression(
        C=10.0, penalty="l1", solver="liblinear",
        max_iter=1000, random_state=42
    ),
    "Decision Tree": DecisionTreeClassifier(
        criterion="gini", max_depth=10, max_features="sqrt",
        min_samples_leaf=10, min_samples_split=5,
        random_state=42
    ),
    "XGBoost": XGBClassifier(
        n_estimators=100, learning_rate=0.20, max_depth=5,
        subsample=0.90, colsample_bytree=0.80,
        random_state=42, eval_metric="logloss",
        use_label_encoder=False
    ),
    "CatBoost": CatBoostClassifier(
        iterations=100, learning_rate=0.20, depth=4,
        l2_leaf_reg=1, random_seed=42, verbose=False
    ),
    "AdaBoost": AdaBoostClassifier(
        n_estimators=100, learning_rate=1.0,
        algorithm="SAMME", random_state=42
    )
}

# ==============================================================
# C)  PAIRED t-TEST FUNCTION
# ==============================================================
def paired_t_test(model_a, model_b, name_a, name_b,
                  X, y, cv_folds=10, scoring="roc_auc"):

    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    s1 = cross_val_score(model_a, X, y, cv=cv, scoring=scoring)
    s2 = cross_val_score(model_b, X, y, cv=cv, scoring=scoring)

    t_stat, p_val = ttest_rel(s1, s2)
    return s1.mean(), s2.mean(), p_val

# ==============================================================
# D)  RUN ALL PAIRWISE COMPARISONS & PRINT SUMMARY
# ==============================================================
alpha = 0.05
summary = []

for (name1, mdl1), (name2, mdl2) in combinations(models.items(), 2):
    m1, m2, p = paired_t_test(mdl1, mdl2, name1, name2, X, y)
    signif = "‚úÖ" if p < alpha else "‚Äî"
    better = name1 if m1 > m2 else name2
    summary.append([name1, name2, m1, m2, p, signif, better])

print("\n=========== PAIRED t-TEST SUMMARY (Œ± = 0.05) ===========")
print(f"{'Model A':20} {'Model B':20} {'A mean':>8} {'B mean':>8} "
      f"{'p-value':>10} {'Sig?':>6} {'Better':>10}")
for row in summary:
    print(f"{row[0]:20} {row[1]:20} {row[2]:8.4f} {row[3]:8.4f} "
          f"{row[4]:10.4f} {row[5]:>6} {row[6]:>10}")

# Clone a public repository
!git clone https://github.com/urilevy77/startups-success-prediction.git

# Navigate to the cloned directory
import os
os.chdir('startups-success-prediction')

!git remote add origin https://github.com/urilevy77/startups-success-prediction.git
!git branch -M main
!git push -u origin main